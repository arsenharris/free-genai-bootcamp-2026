##Running Ollama Third party service


### Choosing a model

You can get the model_id that ollama will launch from the [OllamaLibrary](hhtps://ollama.com/library)

https://ollama.com/library/llama3.2

e.g.https://ollama.com/library/llama3.2


### Getting the Host IP


### Linux
``` sh
sudo apt install 

LLM_ENDPOINT_PORT=8008 LLM_MODEL_ID="llama3.2:1b
host_ip=192.168.1.100 dockercompose
      - ${LLM_ENDPOINT_PORT:-8008}:11434